{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79bbb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Purpose and Benefits of Pooling in CNN:\n",
    "Pooling is a technique used in Convolutional Neural Networks (CNNs) to downsample the spatial dimensions of the input volume, reducing the computational complexity and the number of parameters. The main purposes and benefits of pooling include:\n",
    "Dimension Reduction: Pooling reduces the spatial dimensions (width and height) of the input, making subsequent layers computationally more efficient.\n",
    "Translation Invariance: Pooling provides a degree of translation invariance by considering local features rather than precise spatial locations. This makes the network more robust to variations in object position within the input.\n",
    "Increased Receptive Field: Pooling helps increase the receptive field of higher-layer neurons, enabling them to capture more abstract features.\n",
    "Feature Generalization: Pooling retains the most essential information while discarding less important details, promoting feature generalization.\n",
    "\n",
    "2.Difference between Max Pooling and Average Pooling:\n",
    "Max Pooling: Takes the maximum value from each local region. It emphasizes the most salient features in the region.\n",
    "Average Pooling: Computes the average value of each local region. It provides a smoother downsampling and is less sensitive to outliers.\n",
    "\n",
    "3.Padding in CNN and its Significance:\n",
    "Padding involves adding extra pixels around the input image or feature map before applying convolution or pooling operations. Its significance lies in:\n",
    "Preserving Spatial Information: Padding helps retain the spatial dimensions of the input, preventing a reduction in size after convolution or pooling operations.\n",
    "Avoiding Edge Effects: Without padding, convolutional operations can cause a reduction in spatial dimensions, leading to loss of information at the edges of the input.\n",
    "Facilitating Stride Control: Padding allows for better control over the stride of convolutional operations, ensuring a more fine-grained analysis of the input.\n",
    "\n",
    "4.Comparison of Zero-padding and Valid-padding:\n",
    "Zero-padding: Adds zero values around the input. Preserves spatial dimensions and reduces the impact of edge effects.\n",
    "Valid-padding: Does not add extra pixels. Results in a reduction in spatial dimensions and may lead to information loss at the edges.\n",
    "Effects on Output Feature Map Size:\n",
    "Zero-padding: Maintains the spatial dimensions of the input.\n",
    "Valid-padding: Reduces the spatial dimensions of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9c0ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.LeNet-5, proposed by Yann LeCun and his collaborators in 1998, is one of the pioneering convolutional neural network (CNN) architectures designed for handwritten digit recognition. It played a significant role in the development of deep learning and laid the foundation for more advanced architectures. Here's an overview of its key components:\n",
    "\n",
    "2.Input Layer:\n",
    "LeNet-5 takes grayscale images of size 32x32 as input.\n",
    "Convolutional Layers:\n",
    "The first convolutional layer uses 6 filters of size 5x5, followed by a tanh activation function.\n",
    "The second convolutional layer uses 16 filters of size 5x5, followed by a tanh activation function.\n",
    "Convolutional layers are responsible for feature extraction.\n",
    "Average Pooling Layers:\n",
    "After each convolutional layer, LeNet-5 uses average pooling with a 2x2 window and a stride of 2.\n",
    "Pooling layers reduce spatial dimensions and provide translation invariance.\n",
    "Fully Connected Layers:\n",
    "LeNet-5 has three fully connected layers with 120, 84, and 10 neurons, respectively.\n",
    "Fully connected layers serve as classifiers, converting features into class probabilities.\n",
    "Output Layer:\n",
    "The output layer uses a softmax activation function to produce class probabilities.\n",
    "\n",
    "3.Advantages and Limitations of LeNet-5 in Image Classification Tasks:\n",
    "Advantages:\n",
    "Pioneering Design: LeNet-5 was among the first successful CNN architectures, establishing the importance of convolutional and pooling layers.\n",
    "Effective Feature Extraction: Convolutional and pooling layers help extract hierarchical features from input images.\n",
    "Translation Invariance: Pooling layers contribute to translation invariance, making the network robust to slight shifts in object position.\n",
    "Limitations:\n",
    "Limited Capacity: LeNet-5 may not handle complex datasets with diverse patterns as effectively as more modern architectures.\n",
    "Small Receptive Field: Due to the small filter sizes, LeNet-5 may struggle with capturing larger and more complex patterns in high-resolution images.\n",
    "\n",
    "4.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Define LeNet-5 model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(6, kernel_size=(5, 5), activation='tanh', input_shape=(28, 28, 1)),\n",
    "    layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    layers.Conv2D(16, kernel_size=(5, 5), activation='tanh'),\n",
    "    layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(120, activation='tanh'),\n",
    "    layers.Dense(84, activation='tanh'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=10, batch_size=128, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Test Accuracy: {test_acc}')\n",
    "\n",
    "Insights:\n",
    "LeNet-5 is a relatively simple architecture designed for digit recognition tasks, and MNIST is well-suited for its capabilities.\n",
    "The tanh activation function is used in convolutional and fully connected layers, providing non-linearity.\n",
    "The average pooling layers contribute to downsampling and translation invariance.\n",
    "The model is trained using the Adam optimizer and categorical crossentropy loss.\n",
    "The performance can be evaluated using the test accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c34eb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.AlexNet, designed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, is a landmark convolutional neural network (CNN) architecture that won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. Here's an overview of its key aspects:\n",
    "\n",
    "2.Architectural Innovations:\n",
    "Deep Architecture: AlexNet was one of the first deep neural networks, consisting of eight layers, five convolutional layers, and three fully connected layers. This depth contributed to its ability to learn hierarchical features.\n",
    "Rectified Linear Units (ReLU): AlexNet used the rectified linear unit as the activation function, introducing non-linearity and enabling faster convergence during training.\n",
    "Overlapping Pooling: Instead of traditional non-overlapping pooling, AlexNet employed overlapping max-pooling layers with a stride of 2, reducing the spatial dimensions while preserving more information.\n",
    "Local Response Normalization (LRN): LRN layers were introduced to normalize the responses within a local region, enhancing generalization and making the network more robust to variations in input.\n",
    "Dropout: AlexNet incorporated dropout in the fully connected layers during training, reducing overfitting by randomly dropping units, preventing co-adaptation of hidden units.\n",
    "Data Augmentation: To address the limited size of the ImageNet dataset, AlexNet applied data augmentation techniques during training, such as cropping and flipping, to increase the diversity of training samples.\n",
    "\n",
    "3.Role of Convolutional Layers, Pooling Layers, and Fully Connected Layers:\n",
    "Convolutional Layers:\n",
    "The first convolutional layer had 96 kernels of size 11x11 with a stride of 4.\n",
    "The second and third convolutional layers had 256 and 384 kernels of size 5x5, respectively.\n",
    "The fourth and fifth convolutional layers had 384 and 256 kernels, respectively.\n",
    "Pooling Layers:\n",
    "Overlapping max-pooling layers were applied after the first, second, and fifth convolutional layers.\n",
    "The pooling layers had a 3x3 window and a stride of 2.\n",
    "Fully Connected Layers:\n",
    "The three fully connected layers had 4096 neurons each, leading to a high-dimensional feature representation.\n",
    "The final fully connected layer had 1000 neurons corresponding to the 1000 ImageNet classes.\n",
    "Advantages:\n",
    "Hierarchical Feature Learning: The deep architecture allowed the model to learn hierarchical features, capturing both low-level and high-level representations.\n",
    "ReLU Activation: The use of ReLU helped mitigate the vanishing gradient problem and accelerated convergence during training.\n",
    "Large-Scale Data: AlexNet demonstrated the effectiveness of deep learning on large-scale datasets like ImageNet.\n",
    "Limitations:\n",
    "Computational Intensity: The depth and complexity of AlexNet made it computationally intensive, requiring powerful GPUs for training.\n",
    "Memory Usage: The large number of parameters in fully connected layers increased memory requirements.\n",
    "\n",
    "4.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess the CIFAR-10 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.astype('float32') / 255\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Define AlexNet model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(96, kernel_size=(11, 11), strides=(4, 4), activation='relu', input_shape=(32, 32, 3)),\n",
    "    layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),\n",
    "    layers.Conv2D(256, kernel_size=(5, 5), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),\n",
    "    layers.Conv2D(384, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    layers.Conv2D(384, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    layers.Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(4096, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(4096, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=10, batch_size=128, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Test Accuracy: {test_acc}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
